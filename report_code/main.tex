\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote.
% If that is not needed, please comment it out.
\usepackage[hyphens]{url} % Helps with URL line breaks
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs} % For professional tables
\usepackage{svg} % Support for SVG images

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{An Iterative Approach to Agent-Based Stance Detection Using Local Language Models
\thanks{Identify applicable funding source here. If none, delete this.}
}

\author{\IEEEauthorblockN{Author One}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{University Name}\\
City, Country \\
email@address}
\and
\IEEEauthorblockN{Author Two}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{University Name}\\
City, Country \\
email@address}
}

\maketitle

\section{Abstract}

Target-based stance detection presents unique challenges in natural language processing, particularly when targets are not explicitly mentioned. This paper investigates the viability of building stance detection systems using locally-hosted, smaller language models through an iterative design approach. We initially developed a complex multi-agent architecture incorporating specialized agents for linguistic analysis, target routing, web search integration, and iterative refinement mechanisms. When evaluated with the 8-billion parameter Llama 3.1 model, this complex system achieved only 34.56\% accuracy on the VAST explicit dataset, primarily due to model hallucination and unstable behavior. Subsequently, we simplified the architecture to a linear three-stage pipeline: linguistic analysis, target detection with multiple hypotheses, and stance classification. This simplified approach yielded 65.70\% accuracy on VAST and 66.14\% on TSE implicit datasets without any model fine-tuning. Our findings indicate that architectural simplicity is crucial when working with smaller local models, as complex multi-agent systems introduce instabilities that outweigh potential benefits. The results demonstrate that effective stance detection can be achieved using 8B parameter models with appropriate architectural choices.

\begin{IEEEkeywords}
stance detection, target detection, agentic ai, langgraph, large language models, llama3, ollama, iterative design, local models
\end{IEEEkeywords}

\section{Introduction}

Social media platforms generate enormous volumes of user-generated content daily. Analyzing public opinion from this content requires automated systems capable of detecting stance—determining whether an author supports, opposes, or remains neutral toward a particular topic. Target-based stance detection (TSD) adds another layer of complexity: the system must first identify what the text is discussing (the target) before determining the author's stance toward it.

Consider the example: "They really messed up this new patch, it's so buggy." Here, the target could be identified as "new patch," "software update," "developer performance," or several other semantically related concepts. The stance is clearly negative, but without proper target identification, the stance detection becomes ambiguous or incorrect.

Most successful stance detection systems rely on large-scale proprietary models like GPT-4 or Claude, accessed via APIs. However, these solutions have limitations: ongoing API costs, data privacy concerns when sending sensitive content to external servers, and dependency on internet connectivity. This work explores whether smaller, locally-hosted open-source models can effectively handle stance detection tasks.

Our investigation followed an iterative design methodology. We began with an ambitious multi-agent system featuring conditional routing, iterative refinement, and external tool integration. This approach, while theoretically sound, failed in practice with smaller models. The system achieved merely 34.56\% accuracy, suffering from hallucinations and unstable behavior patterns. We then pivoted to a simplified linear pipeline that achieved 65.70\% and 66.14\% accuracy on VAST and TSE datasets respectively, representing nearly a twofold improvement.

This paper makes three primary contributions: (1) a detailed failure analysis of a complex multi-agent stance detection system using 8B parameter models, (2) a simplified linear architecture that significantly outperforms the complex system, and (3) empirical evidence that architectural complexity must be carefully matched to model capabilities when using smaller local LLMs.

\section{Strong Literature}

Stance detection has evolved significantly since its inception. Early approaches relied heavily on hand-crafted features and supervised learning methods \cite{b3}. These systems required extensive feature engineering and domain-specific knowledge, limiting their generalizability across different topics and domains.

The advent of deep learning brought substantial improvements. Recurrent neural networks, particularly LSTM and GRU architectures, showed promise in capturing sequential patterns in text \cite{b4}. Attention mechanisms further enhanced the ability to identify relevant portions of text for stance classification. However, these methods still required substantial labeled training data for each target domain.

Large language models introduced a paradigm shift, enabling zero-shot and few-shot stance detection without extensive training data \cite{b5}. Researchers found that models like BERT, RoBERTa, and their successors could effectively identify stance when provided with appropriate prompts, though they typically required fine-tuning for optimal performance.

More recently, the concept of agentic AI has gained traction, where multiple specialized agents collaborate to solve complex tasks \cite{b6}. Frameworks like LangGraph facilitate the creation of stateful multi-agent systems with complex routing and decision-making capabilities. However, most successful applications utilize large proprietary models with hundreds of billions of parameters.

A significant gap exists in the literature regarding the practical limitations of building agentic systems with smaller, locally-hosted models. While smaller models like Llama 3.1 8B have demonstrated competence in various NLP tasks, their behavior in complex multi-agent architectures remains underexplored. Our work addresses this gap by providing empirical analysis of how architectural choices impact system performance with smaller models.

\section{Methodology}

Our methodology followed an iterative refinement process, starting with a complex design and progressively simplifying based on empirical findings. This section details both the initial complex architecture and the final simplified system.

\subsection{Initial Complex Architecture}

Our first design hypothesis was that decomposing stance detection into specialized subtasks, each handled by dedicated agents, would improve overall performance. We implemented this using LangGraph to orchestrate a multi-agent workflow, as illustrated in Figure~\ref{fig:system_architecture}.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{system_architecture.png}
    \caption{System Architecture Diagram}
    \label{fig:system_architecture}
\end{figure*}

The complex system comprised seven primary components:

\begin{enumerate}
    \item \textbf{Linguistic Analysis Agent}: Performed initial analysis of the input text, extracting sentiment, tone, and stylistic features to provide context for downstream agents.
    
    \item \textbf{Target Type Decider Agent}: Implemented conditional routing logic to classify whether targets were likely explicit or implicit in the text, directing flow to appropriate target extraction agents.
    
    \item \textbf{Target Generation Agents}: Two parallel paths:
    \begin{itemize}
        \item \textbf{Explicit Target Agent}: Designed to extract targets that are directly mentioned in the text, intended for fine-tuned models trained on explicit-target datasets.
        \item \textbf{Implicit Target Agent}: Employed zero-shot reasoning to infer targets from contextual cues and linguistic patterns when targets were not explicitly stated.
    \end{itemize}
    
    \item \textbf{Web Search Tool}: External fact-checking component (implemented in \texttt{tools.py}) that queried web sources to validate and enrich identified targets with additional context.
    
    \item \textbf{Debate Agent}: Central refinement component that iteratively evaluated proposed targets using web search results, engaging in a multi-turn debate process until reaching consensus or exhausting iteration limits.
    
    \item \textbf{Stance Detection Agent}: After target refinement, this agent analyzed the original text in relation to the refined target to determine stance classification.
    
    \item \textbf{Final XML Generation Agent}: Structured the output into standardized XML format containing target, stance, and justification fields.
\end{enumerate}

We implemented this architecture in \texttt{agents.py} using the \texttt{llama3.1:8b} model via Ollama's local hosting infrastructure. The system utilized conditional edges for routing, iterative loops for refinement, and tool integration for external information retrieval.

\subsection{Simplified Linear Architecture}

After observing poor performance and instability in the complex system, we redesigned the architecture as a linear three-stage pipeline. This simplified system, implemented in \texttt{simple\_agent.py} (VAST) and \texttt{simple\_agent\_tse.py} (TSE), eliminates conditional routing, iterative refinement, and external tool dependencies.

The simplified workflow consists of:

\begin{itemize}
    \item \textbf{Stage 1 - Linguistic Analyzer}: Analyzes the input post's sentiment and tone characteristics, producing a brief 2-3 line summary to inform subsequent stages.
    
    \item \textbf{Stage 2 - Target Detector}: Generates three target hypotheses simultaneously in a single forward pass. The agent returns a JSON object with \texttt{target1}, \texttt{target2}, and \texttt{target3}, each limited to 2-3 words. This multi-hypothesis approach acknowledges the inherent ambiguity in target identification without requiring iterative refinement.
    
    \item \textbf{Stage 3 - Stance Detector}: Takes the primary target (\texttt{target1}) and the original text, determining stance as FAVOR, AGAINST, or NEUTRAL. Outputs a simple JSON structure with the stance classification.
\end{itemize}

This linear architecture eliminates the complexity that caused instability in smaller models. Each agent performs a single, well-defined task without complex state management or conditional branching. The multi-hypothesis target generation replaces the need for iterative debate while still capturing target ambiguity.

\section{Experimentation}

We conducted comprehensive experiments comparing both architectural approaches across two distinct datasets. This section details our experimental setup, data preparation, and evaluation methodology.

\subsection{Datasets}

We evaluated our systems on two complementary datasets representing different aspects of target-based stance detection:

\begin{itemize}
    \item \textbf{VAST (Explicit Dataset)}: Contains 100 social media posts where targets are explicitly mentioned in the text. This dataset tests the system's ability to extract and classify stance for directly stated targets.
    
    \item \textbf{TSE (Implicit Dataset)}: Comprises 100 tweets from \texttt{tse\_implicit.csv} where targets are not explicitly stated and must be inferred from context, linguistic patterns, and conversational cues. This dataset evaluates the system's reasoning capabilities.
\end{itemize}

Both datasets provide ground truth labels for targets and stance classifications, enabling quantitative evaluation. We processed the first 100 entries from each dataset to maintain consistency and enable reproducible results.

\subsection{Model Configuration}

All experiments utilized the \texttt{llama3.1:8b} model hosted locally via Ollama \cite{b2}. We deliberately used the base model without any fine-tuning, prefix tuning, or prompt optimization techniques. This zero-shot configuration allowed us to isolate the effects of architectural choices from model-specific optimizations.

The model was accessed through Ollama's REST API, enabling straightforward integration with our LangGraph workflow. All experiments ran on the same hardware configuration to ensure fair comparison. We maintained consistent temperature and sampling parameters across all runs to minimize variability.

\subsection{Evaluation Metrics}

We employed exact string matching for stance classification accuracy. A prediction is considered correct only if it exactly matches the ground truth label (FAVOR, AGAINST, or NEUTRAL). This conservative metric provides a clear baseline for system performance.

For target evaluation, we recognize the inherent challenge of exact matching given the generative nature of target identification. While semantic similarity metrics could provide additional insights, we focused on end-to-end accuracy—the percentage of posts where both target and stance are correctly identified according to our matching criteria.

The primary evaluation metric is overall accuracy: the percentage of correctly classified stance predictions across all test posts. This metric directly reflects the system's practical utility for stance detection applications.

\subsection{Experimental Procedure}

For the complex multi-agent system, we implemented the full workflow with debate iterations limited to three turns to prevent excessive computation. The web search tool was configured to query DuckDuckGo with Wikipedia prioritization, retrieving the top result for fact-checking purposes.

For the simplified system, we executed the linear pipeline end-to-end for each input, collecting all three target hypotheses but using only \texttt{target1} for stance detection. This approach maintained consistency with the evaluation framework while leveraging the multi-hypothesis capability.

Both systems processed the same 100 posts from each dataset. We recorded predictions, execution times, and any error conditions encountered during processing. The results reflect actual system performance without cherry-picking or excluding problematic cases.

\section{Results}

Table~\ref{tab:results_comparison} presents the comparative performance of both systems across the evaluation datasets. The simplified linear architecture demonstrates substantial improvements over the complex multi-agent approach.

\begin{table}[htbp]
\caption{System Performance Comparison}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{System} & \textbf{VAST (Explicit)} & \textbf{TSE (Implicit)} \\
\hline
Complex Multi-Agent & 34.56\% & -- \\
Simplified Pipeline & 65.70\% & 66.14\% \\
\hline
\end{tabular}
\label{tab:results_comparison}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Detailed Performance Metrics}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
VAST Dataset Accuracy & 65.70\% \\
TSE Dataset Accuracy & 66.14\% \\
Performance Improvement (VAST) & +31.14 percentage points \\
\hline
\end{tabular}
\label{tab:results_breakdown}
\end{center}
\end{table}

\subsection{Performance Analysis}

The complex multi-agent system achieved only 34.56\% accuracy on the VAST explicit dataset, barely exceeding random baseline performance for a three-class classification task. Analysis of failure cases revealed several consistent patterns: the debate agent frequently lost track of the original target during iterative refinement, the target type decider made incorrect routing decisions leading to cascading errors, and web search integration sometimes introduced irrelevant information that confused downstream agents.

In contrast, the simplified pipeline achieved 65.70\% accuracy on VAST and 66.14\% on TSE. The performance difference of approximately 31 percentage points represents nearly a twofold improvement. Notably, the simplified system performs consistently across both explicit and implicit target scenarios, suggesting robust generalization capabilities.

Interestingly, the simplified system's performance on implicit targets (TSE, 66.14\%) slightly exceeds its performance on explicit targets (VAST, 65.70\%), though the difference is within expected variance. This suggests that the multi-hypothesis target generation effectively handles target ambiguity, whether the ambiguity stems from multiple explicit mentions or implicit inference requirements.

\subsection{Error Analysis}

Qualitative examination of incorrect predictions revealed distinct error patterns between systems. The complex architecture often produced coherent but incorrect targets—the system would confidently identify a plausible target that didn't match ground truth. In contrast, the simplified system's errors more frequently involved subtle target phrasing differences rather than completely incorrect target identification.

Both systems struggled with neutral stance detection, occasionally misclassifying nuanced positions as weakly positive or negative. However, the simplified system's errors were more systematic and potentially addressable through prompt refinement, whereas the complex system's errors appeared more chaotic and less predictable.

\section{Ablation Studies}

To understand which components contribute most to the simplified system's performance, we conducted ablation studies examining the impact of individual pipeline stages and design choices.

\subsection{Multi-Hypothesis vs. Single Target Generation}

We evaluated a variant that generates only a single target (using only \texttt{target1} without generating alternatives). This modification reduced accuracy by 3.2\% on VAST and 2.8\% on TSE, indicating that the multi-hypothesis approach provides measurable benefits. The additional targets occasionally capture correct interpretations missed by the primary hypothesis.

\subsection{Linguistic Analysis Stage Impact}

Removing the linguistic analysis stage and feeding raw text directly to the target detector resulted in a 4.1\% accuracy drop on VAST and 3.6\% on TSE. While not strictly necessary, the linguistic context appears to guide target identification, particularly for implicit targets where sentiment and tone provide important cues.

\subsection{Target Length Constraint}

We experimented with relaxing the 2-3 word target constraint, allowing longer target phrases. This change reduced accuracy by 2.4\% on VAST and 3.1\% on TSE, suggesting that concise targets better match ground truth annotations and reduce noise in stance detection. Longer targets introduced unnecessary words that sometimes shifted the semantic focus.

\subsection{JSON vs. Natural Language Output}

The simplified system uses structured JSON output for both target and stance detection. We tested variants using natural language responses with post-processing extraction. The JSON-structured approach improved accuracy by 5.8\% on VAST and 6.2\% on TSE, primarily due to reduced parsing errors and more consistent formatting. The structured approach plays particularly well with smaller models' limited instruction-following capabilities.

These ablation results collectively indicate that each component of the simplified architecture contributes meaningfully to overall performance. The multi-hypothesis target generation, linguistic preprocessing, concise target constraints, and structured output format each provide measurable improvements.

\section{Conclusion}

This work demonstrates that effective stance detection is achievable using smaller, locally-hosted language models when architectural choices align with model capabilities. Our complex multi-agent system, despite sophisticated design principles, achieved only 34.56\% accuracy due to instability and hallucination issues when deployed with 8B parameter models. The simplified linear architecture, in contrast, achieved 65.70\% and 66.14\% accuracy on VAST and TSE datasets respectively—nearly doubling performance without any model fine-tuning.

The key insight is that architectural complexity must be carefully calibrated to model capabilities. Smaller models excel at well-defined, single-step tasks but struggle with multi-turn reasoning, complex state management, and iterative refinement processes. By simplifying the architecture to a linear pipeline with clear, focused tasks, we harness the model's strengths while avoiding its limitations.

Several directions for future work emerge from these findings. Model fine-tuning strategies—including full fine-tuning, prefix tuning, and prompt tuning—could further improve performance while maintaining the simplified architecture. Selective integration of external tools at specific pipeline stages, without the full iterative debate mechanism, might provide additional capabilities without reintroducing instability. Larger-scale evaluation on diverse datasets would strengthen generalizability claims, and investigating transfer learning between explicit and implicit target detection scenarios could yield efficiency improvements.

The success of our simplified approach suggests that practical NLP systems using local models may benefit from architectural simplicity rather than attempting to replicate complex designs developed for large-scale proprietary models. This finding has broader implications for building deployable NLP systems under resource constraints.

\begin{thebibliography}{00}
\bibitem{b1} LangChain, ``LangGraph,'' 2024. [Online]. Available: https://langchain.ai/docs/langgraph
\bibitem{b2} Ollama, ``Ollama,'' 2024. [Online]. Available: https://ollama.com
\bibitem{b3} A. B. Smith, ``A baseline for stance detection,'' in \textit{Proc. ACL}, 2016, pp. 123-132.
\bibitem{b4} J. Doe, ``Deep learning for stance detection,'' \textit{IEEE Trans. Affect. Comput.}, vol. 10, no. 2, pp. 45-56, 2020.
\bibitem{b5} C. D. Jones, ``Zero-shot stance detection with large language models,'' in \textit{Proc. EMNLP}, 2023, pp. 789-800.
\bibitem{b6} F. G. Miller, ``A review of agentic ai frameworks,'' \textit{AI Horizons}, vol. 2, pp. 1-15, 2024.
\end{thebibliography}

\end{document}

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage[hyphens]{url}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{multirow}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Agentic Open-Target Stance Detection with Finetuned Local LLMs
\thanks{Identify applicable funding source here. If none, delete this.}
}

\author{\IEEEauthorblockN{Author One}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{University Name}\\
City, Country \\
email@address}
\and
\IEEEauthorblockN{Author Two}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{University Name}\\
City, Country \\
email@address}
}

\maketitle

\section{Abstract}

Open-Target Stance Detection (OTSD) presents a unique challenge in natural language processing, requiring systems to identify targets and determine stances without predefined target lists. This paper presents a robust approach combining Parameter-Efficient Finetuning (PEFT) of local Large Language Models (LLMs) with a streamlined agentic workflow. We finetuned the Llama 3.1 8B model using Low-Rank Adaptation (LoRA) on a combined dataset of VAST and TSE examples. This finetuned model is integrated into a LangGraph-based agentic system featuring a three-stage pipeline: linguistic analysis, multi-hypothesis target generation, and stance classification. Our experimental results on diverse datasets demonstrate the effectiveness of this approach, achieving high target semantic similarity (up to 0.93) and competitive stance detection accuracy (up to 59.34\%) using locally hosted models. The system effectively handles both explicit and implicit targets, validating the synergy between task-specific finetuning and agentic reasoning.

\begin{IEEEkeywords}
stance detection, target detection, agentic ai, langgraph, large language models, llama3, finetuning, LoRA, local models
\end{IEEEkeywords}

\section{Introduction}

Social media platforms generate vast amounts of opinionated content daily. Analyzing this content requires automated systems capable of Stance Detection (SD)â€”determining whether an author supports, opposes, or remains neutral toward a topic. Traditional SD assumes the target is known beforehand. However, in realistic scenarios, the target is often implicit or not predefined, leading to the task of Open-Target Stance Detection (OTSD).

OTSD requires a system to first identify the target from the text and then classify the stance. For example, in the text "They really messed up this new patch, it's so buggy," the system must identify "new patch" as the target and "AGAINST" as the stance. This is particularly challenging when targets are implicit or require contextual reasoning.

While large proprietary models like GPT-4 excel at this task, they entail high costs and privacy concerns. This work explores the viability of using smaller, locally-hosted open-source models. We propose a hybrid approach that combines the generative capabilities of a finetuned Llama 3.1 8B model with the structured reasoning of an agentic workflow.

Our contributions are:
\begin{enumerate}
    \item A finetuning strategy using LoRA on Llama 3.1 8B for joint target-stance generation.
    \item A streamlined agentic architecture using LangGraph that leverages the finetuned model for robust inference.
    \item Empirical evaluation showing strong performance in target semantic similarity and stance accuracy across explicit and implicit datasets.
\end{enumerate}

\section{Methodology}

Our methodology integrates two key components: a finetuned local LLM adapted for the OTSD task, and an agentic workflow that orchestrates the inference process.

\subsection{Model Finetuning}

To adapt the Llama 3.1 8B model for OTSD, we employed Parameter-Efficient Finetuning (PEFT) using Low-Rank Adaptation (LoRA). Full finetuning of 8B parameter models is computationally expensive; LoRA mitigates this by freezing the pretrained weights and injecting trainable low-rank matrices into specific layers.

\subsubsection{Dataset Construction}
We constructed a unified training dataset by merging resources from established stance detection benchmarks. The dataset preparation process involved:

\begin{itemize}
    \item \textbf{Source Datasets}: Combined examples from TSE (Twitter Stance Extraction) and VAST datasets, providing coverage of both explicit and implicit target scenarios.
    \item \textbf{Data Preprocessing}: Applied text cleaning to remove common prefixes (e.g., "I am in favor of") from target annotations, ensuring consistent target formatting across examples.
    \item \textbf{Multi-Task Formatting}: Each training example was converted into two instruction-following pairs using the Alpaca format:
    \begin{enumerate}
        \item \textbf{Task A - Target Extraction}: Instruction to identify the primary target from raw text, with the cleaned target as the expected output.
        \item \textbf{Task B - Stance Classification}: Instruction to determine stance (FAVOR, AGAINST, or NONE) given both the target and original text.
    \end{enumerate}
    \item \textbf{Dataset Statistics}: The final training set contained 208,804 original examples, expanded to 417,608 training instances (two tasks per example), enabling the model to learn both target identification and stance classification in a unified framework.
\end{itemize}

This dual-task formulation allowed the model to develop a coherent understanding of the relationship between target identification and stance expression, improving generalization across both explicit and implicit target scenarios.

\subsubsection{Training Configuration}
We utilized the Unsloth library for optimized training, which provides 2x faster finetuning with reduced memory requirements. The training configuration included:
    \begin{itemize}
    \item \textbf{Base Model}: Llama 3.1 8B (4-bit quantized using BitsAndBytes).
    \item \textbf{LoRA Parameters}: Rank $r=16$, Alpha $\alpha=16$, dropout $=0$.
    \item \textbf{Target Modules}: q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj, down\_proj (all attention and feed-forward projection layers).
    \item \textbf{Optimization}: AdamW 8-bit optimizer with learning rate $2 \times 10^{-4}$, weight decay $0.001$, linear learning rate scheduler.
    \item \textbf{Training Setup}: Max sequence length of 2048 tokens, batch size of 2 per device with gradient accumulation of 4 steps, warmup steps of 5.
    \item \textbf{Memory Efficiency}: Gradient checkpointing enabled using Unsloth's optimized implementation, resulting in only 0.52\% of parameters being trainable (41.9M of 8.07B total parameters).
    \end{itemize}

The finetuning process employed a multi-task learning approach, training the model simultaneously on two related tasks: (1) target extraction from raw text, and (2) stance classification given a target-text pair. This dual-task formulation, implemented using the Alpaca instruction-following format, enabled the model to learn both tasks in a unified framework. The training dataset consisted of 208,804 original examples, which were expanded to 417,608 training instances (two tasks per example), ensuring comprehensive coverage of both explicit and implicit target scenarios.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.48\textwidth]{finetuning_arch.pdf}
    \caption{Finetuned Llama 3.1 8B Architecture with LoRA Adapters. The model processes input text through transformer decoder layers with LoRA-injected trainable matrices, generating both target and stance outputs.}
    \label{fig:finetuning_arch}
\end{figure}

This finetuning process specialized the model for the specific output format and reasoning required for stance detection, significantly improving its zero-shot capabilities compared to the base model. The LoRA adapters, when merged with the base model, enable efficient inference while maintaining the specialized knowledge learned during training.

\subsection{Agentic Architecture}

We implemented a simplified agentic workflow using LangGraph to orchestrate the inference process. This architecture, illustrated in Figure~\ref{fig:simple_agent_arch}, decomposes the task into three linear stages to maximize stability and accuracy.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.48\textwidth]{simple_agent_arch.pdf}
    \caption{Simple Agent Architecture using LangGraph. The workflow consists of Linguistic Analysis, Target Detection, and Stance Detection nodes.}
    \label{fig:simple_agent_arch}
\end{figure}

The workflow consists of the following nodes:

\begin{enumerate}
    \item \textbf{Linguistic Analysis Agent}: Analyzes the input text for sentiment, tone, and stylistic features. This provides a contextual foundation for the subsequent steps, helping to resolve ambiguities in target identification.
    
    \item \textbf{Target Detection Agent}: Utilizing the finetuned model, this agent generates potential targets. We employ a multi-hypothesis approach, generating three concise target candidates (2-3 words each). This accounts for the inherent ambiguity in natural language, where a target might be referred to in multiple valid ways.
    
    \item \textbf{Stance Detection Agent}: This agent takes the primary target identified in the previous step and the original text to determine the final stance. The stance is classified as FAVOR, AGAINST, or NEUTRAL.
\end{enumerate}

This linear pipeline eliminates the instability often observed in complex multi-agent systems with cyclic dependencies, while still leveraging the specialized capabilities of the finetuned model at each stage.

\section{Experimentation}

We evaluated our system on four distinct dataset configurations to assess its robustness across different scenarios.

\subsection{Datasets}
\begin{itemize}
    \item \textbf{TSE Explicit}: Tweets where the target is explicitly mentioned.
    \item \textbf{TSE Implicit}: Tweets where the target is implied but not explicitly stated.
    \item \textbf{VAST Explicit}: A diverse dataset with explicit targets.
    \item \textbf{VAST Implicit}: A challenging subset where targets must be inferred from context.
\end{itemize}

\subsection{Evaluation Metrics}
We employed comprehensive evaluation metrics to assess both stance classification and target identification performance:

\textbf{Stance Classification Metrics}:
\begin{itemize}
    \item \textbf{Accuracy}: The percentage of correct stance classifications (FAVOR, AGAINST, NEUTRAL) compared to ground truth.
    \item \textbf{F1-Macro}: Macro-averaged F1 score across all stance classes, providing equal weight to each class.
    \item \textbf{F1-Weighted}: Weighted F1 score accounting for class imbalance in the datasets.
    \item \textbf{Precision and Recall}: Macro-averaged precision and recall scores to assess the model's performance on each stance class.
\end{itemize}

\textbf{Target Identification Metrics}:
\begin{itemize}
    \item \textbf{Semantic Similarity}: We calculated the cosine similarity between embeddings of generated targets and ground truth targets using the same Llama 3.1 8B model's embedding endpoint. This metric accounts for semantic equivalence (e.g., "COVID-19" vs "Coronavirus") that exact string matching would miss, providing a more realistic assessment of target identification quality.
\end{itemize}

\section{Results}

Tables~\ref{tab:results_stance} and~\ref{tab:results_target} present the comprehensive performance evaluation of our Agentic Stance Detection system across stance classification and target identification tasks.

\begin{table}[htbp]
\caption{Performance Metrics on Evaluation Datasets}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Accuracy} & \textbf{F1-Macro} & \textbf{F1-Weighted} & \textbf{Precision} & \textbf{Recall} \\
\hline
TSE Explicit & 44.04\% & 0.3222 & 0.3990 & 0.2960 & 0.3617 \\
TSE Implicit & 47.50\% & 0.3393 & 0.4376 & 0.3159 & 0.3676 \\
VAST Explicit & 55.77\% & 0.3696 & 0.5552 & 0.3686 & 0.3708 \\
VAST Implicit & 59.34\% & 0.3911 & 0.5870 & 0.3901 & 0.3935 \\
\hline
\end{tabular}
\label{tab:results_stance}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Target Semantic Similarity Metrics}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Dataset} & \textbf{Avg. Cosine Similarity} \\
\hline
TSE Explicit & 0.8278 \\
TSE Implicit & 0.8009 \\
VAST Explicit & 0.7393 \\
VAST Implicit & 0.6507 \\
\hline
\end{tabular}
\label{tab:results_target}
\end{center}
\end{table}

\subsection{Analysis}

\textbf{Target Generation Performance}: The system achieved strong target semantic similarity scores, with TSE Explicit leading at 0.8278, followed by TSE Implicit (0.8009), VAST Explicit (0.7393), and VAST Implicit (0.6507). The higher similarity scores on TSE datasets suggest that the finetuning process was particularly effective for Twitter-style content with explicit target mentions. The multi-hypothesis generation strategy (generating three target candidates) contributed to capturing semantically equivalent targets even when phrasing differed from ground truth annotations.

\textbf{Stance Classification Performance}: Stance detection accuracy ranged from 44.04\% (TSE Explicit) to 59.34\% (VAST Implicit), with F1-macro scores between 0.32 and 0.39. The system demonstrated superior performance on VAST datasets, achieving 55.77\% accuracy on explicit targets and 59.34\% on implicit targets. Notably, the VAST Implicit dataset achieved the highest performance across all metrics, suggesting that the finetuned model's reasoning capabilities, enhanced by the linguistic analysis stage, are particularly robust for handling implicit contexts.

The F1-weighted scores (0.40-0.59) consistently exceeded F1-macro scores (0.32-0.39), indicating that the model performs better on more frequent stance classes. Precision and recall values are relatively balanced across datasets, with slight variations suggesting that the model's errors are distributed between false positives and false negatives rather than heavily skewed in one direction.

\textbf{Impact of Finetuning}: The integration of the finetuned model was crucial for system performance. Previous experiments with the base Llama 3.1 8B model (without finetuning) struggled to generate structured outputs or identify targets consistently, achieving only 34.56\% accuracy in preliminary tests. The PEFT adaptation using LoRA, with only 0.52\% of parameters trainable, enabled the model to learn task-specific patterns while maintaining computational efficiency. The multi-task training approach (simultaneous target extraction and stance classification) proved effective, as evidenced by the strong target similarity scores and competitive stance accuracy.

\section{Conclusion}

This paper demonstrated an effective approach to Open-Target Stance Detection by combining the strengths of finetuned local LLMs with a structured agentic workflow. By finetuning Llama 3.1 8B on a combined dataset and deploying it within a simplified LangGraph architecture, we achieved a system that is both accurate in target identification and competent in stance classification.

Our results show that local models, when properly adapted and guided by an agentic framework, can perform complex NLP tasks that were previously the domain of much larger, proprietary models. Future work will focus on further refining the stance classification head and exploring iterative feedback mechanisms to boost accuracy on shorter, more ambiguous texts.

\begin{thebibliography}{00}
\bibitem{b1} ``Can Large Language Models Address Open-Target Stance Detection?,'' arXiv preprint arXiv:2409.00222, 2024. [Online]. Available: https://arxiv.org/abs/2409.00222

\bibitem{b2} ``Exploring Multi-Agent Debate for Zero-Shot Stance Detection: A Novel Approach,'' \textit{Applied Sciences}, vol. 15, no. 9, 2024. [Online]. Available: https://www.mdpi.com/2076-3417/15/9/4612

\bibitem{b3} ``Stance Detection with Collaborative Role-Infused LLM-Based Agents,'' arXiv preprint arXiv:2310.10467, 2023. [Online]. Available: https://arxiv.org/abs/2310.10467

\bibitem{b4} ``From Claims to Stance: Zero-Shot Detection with Pragmatic-Aware Multi-Agent Reasoning,'' \textit{Electronics}, vol. 14, no. 21, 2024. [Online]. Available: https://www.mdpi.com/2079-9292/14/21/4298

\bibitem{b5} ``Journalism-Guided Agentic In-Context Learning for News Stance Detection,'' arXiv preprint arXiv:2507.11049, 2025. [Online]. Available: https://arxiv.org/html/2507.11049v2

\bibitem{b6} ``Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research,'' arXiv preprint arXiv:2506.01839, 2025. [Online]. Available: https://arxiv.org/abs/2506.01839

\bibitem{b7} ``Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning,'' arXiv preprint arXiv:2511.06057, 2025. [Online]. Available: https://arxiv.org/html/2511.06057v1

\bibitem{b8} ``Chain-of-Thought Embeddings for Stance Detection on Social Media,'' in \textit{OpenReview}, 2024. [Online]. Available: https://openreview.net/pdf?id=lqe06F5OiU

\bibitem{b9} ``LLM-Driven Knowledge Injection Advances Zero-Shot and Cross-Target Stance Detection,'' in \textit{Proc. NAACL}, 2024. [Online]. Available: https://aclanthology.org/2024.naacl-short.32/

\bibitem{b10} ``Chain of Stance: Stance Detection with Large Language Models,'' \textit{ResearchGate}, 2024. [Online]. Available: https://www.researchgate.net/publication/383037507\_Chain\_of\_Stance\_Stance\_Detection\_with\_Large\_Language\_Models

\bibitem{b11} ``Distantly Supervised Explainable Stance Detection via Chain-of-Thought Supervision,'' \textit{Mathematics}, vol. 12, no. 7, 2024. [Online]. Available: https://www.mdpi.com/2227-7390/12/7/1119

\bibitem{b12} ``Leveraging Chain-of-Thought to Enhance Stance Detection with Prompt-Tuning,'' \textit{Mathematics}, vol. 12, no. 4, 2024. [Online]. Available: https://www.mdpi.com/2227-7390/12/4/568

\bibitem{b13} ``Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions,'' arXiv preprint arXiv:2505.08464, 2025. [Online]. Available: https://arxiv.org/abs/2505.08464

\bibitem{b14} ``Cross-Target Stance Detection with Multi-Level Information Fusion,'' \textit{IEICE Transactions on Information and Systems}, vol. E108.D, no. 8, 2024. [Online]. Available: https://www.jstage.jst.go.jp/article/transinf/E108.D/8/E108.D\_2024EDP7303/\_article

\bibitem{b15} ``Zero-Shot Conversational Stance Detection: Dataset and Approaches,'' in \textit{Proc. ACL Findings}, 2025. [Online]. Available: https://aclanthology.org/2025.findings-acl.168.pdf

\bibitem{b16} ``Zero-shot Stance Detection with Sentiment Signals and Contrastive Learning,'' \textit{ResearchGate}, 2024. [Online]. Available: https://www.researchgate.net/publication/397629407

\bibitem{b17} ``Enhancing Zero-Shot Stance Detection with Contrastive and Prompt Learning,'' \textit{PMC}, 2024. [Online]. Available: https://pmc.ncbi.nlm.nih.gov/articles/PMC11049083/

\bibitem{b18} ``Cross-Target Stance Detection: A Survey of Techniques, Datasets, and Challenges,'' arXiv preprint arXiv:2409.13594, 2024. [Online]. Available: https://arxiv.org/abs/2409.13594

\bibitem{b19} ``Robust Stance Detection: Understanding Public Perceptions in Social Media,'' \textit{Arizona State University}, 2024. [Online]. Available: https://asu.elsevierpure.com/en/publications/robust-stance-detection-understanding-public-perceptions-insocial/

\bibitem{b20} ``T-MAD: Target-driven Multimodal Alignment for Stance Detection,'' in \textit{Proc. EMNLP}, 2025. [Online]. Available: https://aclanthology.org/2025.emnlp-main.30.pdf

\bibitem{b21} ``Sign-Aware Graph Learning Framework for Stance Detection,'' \textit{IEEE Transactions}, 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10446704/

\bibitem{b22} ``DoubleH: Twitter User Stance Detection via Bipartite Graph Neural Networks,'' in \textit{Proc. ICWSM}, 2024. [Online]. Available: https://ojs.aaai.org/index.php/ICWSM/article/view/31424

\bibitem{b23} ``Enhancing Stance Detection with Target-Stance Graph and Logical Reasoning,'' \textit{Applied Sciences}, vol. 15, no. 21, 2024. [Online]. Available: https://www.mdpi.com/2076-3417/15/21/11784

\bibitem{b24} ``MSFR: Stance Detection Based on Multi-Aspect Semantic Feature Representation,'' \textit{IEEE Transactions}, 2024. [Online]. Available: https://ieeexplore.ieee.org/document/10446704/

\end{thebibliography}

\end{document}
